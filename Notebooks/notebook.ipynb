{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd47485-bd89-439d-b5eb-16f05d5bc750",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 332,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_group\n",
      "variant    7653\n",
      "control    7630\n",
      "Name: count, dtype: int64\n",
      "\n",
      "SRM\n",
      "p-value: 0.8524\n",
      "\n",
      "Primary metric\n",
      "p-value:  0.0002 | effect size:  0.1422\n",
      "\n",
      "Guardrail\n",
      "p-value: 0.5365 | effect size: -0.0079\n"
     ]
    }
   ],
   "source": [
    "# IMPORT PACKAGES\n",
    "import pandas as pd\n",
    "from scipy.stats import chisquare\n",
    "from pingouin import ttest\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# DEFINE FUNCTIONS\n",
    "def estimate_effect_size(df: pd.DataFrame, metric: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate relative effect size\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): data with experiment_group ('control', 'variant') and metric columns.\n",
    "    - metric (str): name of the metric column\n",
    "\n",
    "    Returns:\n",
    "    - effect_size (float): average treatment effect (effect size)\n",
    "    \"\"\"\n",
    "    avg_metric_per_group = df.groupby('experiment_group')[metric].mean()\n",
    "    effect_size = avg_metric_per_group['variant'] / avg_metric_per_group['control'] - 1\n",
    "    return effect_size\n",
    "\n",
    "# FIXED PARAMETERS\n",
    "confidence_level = 0.90  # Set desired confidence level (90%)\n",
    "alpha = 1 - confidence_level  # Significance level for hypothesis tests\n",
    "\n",
    "# LOAD DATA\n",
    "users = pd.read_csv('../Data/users_data.csv') # Load user and experiment group data\n",
    "sessions = pd.read_csv('../Data/sessions_data.csv') # Load session/booking data\n",
    "\n",
    "# JOIN DATA\n",
    "# Merge on user ID to enrich sessions with user experiment group\n",
    "sessions_x_users = sessions.merge(users, on = 'user_id', how = 'inner')\n",
    "\n",
    "# COMPUTE PRIMARY METRIC\n",
    "# Binary conversion flag: 1 if booking occurred, 0 otherwise\n",
    "sessions_x_users['conversion'] = sessions_x_users['booking_timestamp'].notnull().astype(int)\n",
    "\n",
    "# SAMPLE RATIO MISMATCH TEST\n",
    "# Check if the number of users in each experiment group is balanced (a basic A/A sanity check)\n",
    "groups_count = sessions_x_users['experiment_group'].value_counts()\n",
    "print(groups_count)\n",
    "\n",
    "n = sessions_x_users.shape[0] # Total sample size\n",
    "srm_chi2_stat, srm_chi2_pval = chisquare(f_obs = groups_count, f_exp = (n/2, n/2))\n",
    "srm_chi2_pval = round(srm_chi2_pval, 4)\n",
    "print(f'\\nSRM\\np-value: {srm_chi2_pval}') # If p < alpha, there's likely a sampling issue issue\n",
    "    \n",
    "# EFFECT ANALYSIS - PRIMARY METRIC\n",
    "# Compute success counts and sample sizes for each group\n",
    "success_counts = sessions_x_users.groupby('experiment_group')['conversion'].sum().loc[['control', 'variant']]\n",
    "sample_sizes = sessions_x_users['experiment_group'].value_counts().loc[['control', 'variant']]\n",
    "\n",
    "# Run Z-test for proportions (binary conversion metric)\n",
    "zstat_primary, pval_primary = proportions_ztest(\n",
    "    success_counts,\n",
    "    sample_sizes,\n",
    "    alternative = 'two-sided',\n",
    ")\n",
    "pval_primary = round(pval_primary, 4)\n",
    "\n",
    "# Estimate effect size for the conversion metric\n",
    "effect_size_primary = estimate_effect_size(sessions_x_users, 'conversion')\n",
    "effect_size_primary = round(effect_size_primary, 4)\n",
    "print(f'\\nPrimary metric\\np-value: {pval_primary: .4f} | effect size: {effect_size_primary: .4f}')\n",
    "    \n",
    "# EFFECT ANALYSIS - GUARDRAIL METRIC\n",
    "# T-test on time to booking for control vs variant\n",
    "stats_guardrail = ttest(\n",
    "    sessions_x_users.loc[(sessions_x_users['experiment_group'] == 'control'), 'time_to_booking'],\n",
    "    sessions_x_users.loc[(sessions_x_users['experiment_group'] == 'variant'), 'time_to_booking'],\n",
    "    alternative='two-sided',\n",
    ")\n",
    "pval_guardrail, tstat_guardrail = stats_guardrail['p-val'].values[0], stats_guardrail['T'].values[0]\n",
    "pval_guardrail = round(pval_guardrail, 4)\n",
    "\n",
    "# Estimate effect size for the guardrail metric\n",
    "effect_size_guardrail = estimate_effect_size(sessions_x_users, 'time_to_booking')\n",
    "effect_size_guardrail = round(effect_size_guardrail, 4)\n",
    "print(f'\\nGuardrail\\np-value: {pval_guardrail} | effect size: {effect_size_guardrail}')\n",
    "\n",
    "# DECISION\n",
    "# Primary metric must be statistically significant and show positive effect (increase)\n",
    "criteria_full_on_primary = (pval_primary < alpha) & (effect_size_primary > 0)\n",
    "\n",
    "# Guardrail must either be statistically insignificant or whow positive effect (decrease)\n",
    "criteria_full_on_guardrail = (pval_guardrail > alpha) | (effect_size_guardrail <= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b880114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The experiment results are significantly positive and the guardrail metric was not harmed, we are going full on!\n"
     ]
    }
   ],
   "source": [
    "# Final launch decision based on both metrics\n",
    "if criteria_full_on_primary and criteria_full_on_guardrail:\n",
    "    decision_full_on = 'Yes'\n",
    "    print('\\nThe experiment results are significantly positive and the guardrail metric was not harmed, we are going full on!')\n",
    "else:\n",
    "    decision_full_on = 'No'\n",
    "    print('\\nThe experiment results are inconclusive or the guardrail metric was harmed, we are pulling back!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "python_all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
